{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    pipeline, \n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM, \n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set up matplotlib for Arabic text\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Arial'  # Or another font that supports Arabic\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial']  # Ensure Arabic characters display correctly\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"BounharAbdelaziz/xlm-roberta-large-bs-16-lr-0.0001-ep-1-wp-0.1-gacc-8-gnm-1.0-FP16-mx-512-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: BounharAbdelaziz/xlm-roberta-large-bs-16-lr-0.0001-ep-1-wp-0.1-gacc-8-gnm-1.0-FP16-mx-512-v0.1\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL}\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create masked language modeling pipeline\n",
    "mlm_pipeline = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_mlm(mlm_pipeline):\n",
    "    \"\"\"\n",
    "    Load a masked language model and test it on Moroccan Arabic examples.\n",
    "    Args:\n",
    "        model_name (str): HuggingFace model name/path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test examples in Moroccan Arabic\n",
    "    test_sentences = [\n",
    "        \"Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ <mask> Ù‡ÙŠ Ø§Ù„Ø±Ø¨Ø§Ø·\",  # Hello, how are you?\n",
    "        \"Ø§Ù„Ù…ØºØ±Ø¨ <mask> Ø²ÙˆÙŠÙ†\",      # Good morning\n",
    "        \"Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† Ù<mask> Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†\"     # I am at home\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting model on Moroccan Arabic sentences:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        results = mlm_pipeline(sentence, top_k=3)\n",
    "        \n",
    "        print(f\"\\nInput: {sentence}\")\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for result in results:\n",
    "            print(f\"- {result['token_str']}: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ Ù‡ÙŠ Ø§Ù„Ø±Ø¨Ø§Ø·\",  # Hello, how are you?\n",
    "    \" Ø§Ù„Ù…ØºØ±Ø¨ Ø¨Ù„Ø§Ø¯ Ø²ÙˆÙŠÙ†\",      # Good morning\n",
    "    \"Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† ÙØ§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†\"     # I am at home\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model on Moroccan Arabic sentences:\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ <mask> Ù‡ÙŠ Ø§Ù„Ø±Ø¨Ø§Ø·\n",
      "Top 3 predictions:\n",
      "- Ø§Ù„Ù…ØºØ±Ø¨: 0.964\n",
      "- Ø§Ù„Ù…Ù…Ù„ÙƒØ©: 0.007\n",
      "- Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±: 0.006\n",
      "\n",
      "Input: Ø§Ù„Ù…ØºØ±Ø¨ <mask> Ø²ÙˆÙŠÙ†\n",
      "Top 3 predictions:\n",
      "- Ø¨Ù„Ø§Ø¯: 0.781\n",
      "- Ø¨Ù„Ø¯: 0.036\n",
      "- Ù…ÙƒØ§Ù†: 0.031\n",
      "\n",
      "Input: Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† Ù<mask> Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†\n",
      "Top 3 predictions:\n",
      "- Ø§Ù„Ù‚Ø¯Ø³: 0.771\n",
      "- Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©: 0.025\n",
      "- ÙÙ„Ø³Ø·ÙŠÙ†: 0.025\n"
     ]
    }
   ],
   "source": [
    "load_and_test_mlm(mlm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMEmbedder:\n",
    "    def __init__(self, model_name: str = \"UBC-NLP/arabertv02-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the MLM embedding model\n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name/path\n",
    "        \"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Load model without MLM head using AutoModel instead of AutoModelForMaskedLM\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def get_embeddings(\n",
    "        self, \n",
    "        texts: Union[str, List[str]], \n",
    "        pooling_strategy: str = 'mean',\n",
    "        batch_size: int = 32\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for input texts\n",
    "        \n",
    "        Args:\n",
    "            texts: Single text or list of texts\n",
    "            pooling_strategy: How to combine token embeddings ('mean', 'cls', or 'max')\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encoded = self.tokenizer(batch_texts,\n",
    "                                   padding=True,\n",
    "                                   truncation=True,\n",
    "                                   max_length=512,\n",
    "                                   return_tensors='pt')\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask)\n",
    "                \n",
    "                # Get hidden states\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                \n",
    "                # Apply pooling strategy\n",
    "                if pooling_strategy == 'cls':\n",
    "                    # Use [CLS] token embedding\n",
    "                    embeddings = hidden_states[:, 0, :]\n",
    "                    \n",
    "                elif pooling_strategy == 'mean':\n",
    "                    # Mean pooling - take attention mask into account for averaging\n",
    "                    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                    sum_embeddings = torch.sum(hidden_states * attention_mask_expanded, 1)\n",
    "                    sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "                    embeddings = sum_embeddings / sum_mask\n",
    "                    \n",
    "                elif pooling_strategy == 'max':\n",
    "                    # Max pooling\n",
    "                    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                    hidden_states[attention_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "                    embeddings = torch.max(hidden_states, 1)[0]\n",
    "                \n",
    "                # Move to CPU and convert to numpy\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "                all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        return final_embeddings\n",
    "    \n",
    "    def similarity(self, text1: str, text2: str, pooling_strategy: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two texts\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        emb1, emb2 = self.get_embeddings([text1, text2], pooling_strategy=pooling_strategy)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        return similarity\n",
    "    \n",
    "    def compute_similarity_matrix(self, texts: List[str], pooling_strategy: str = 'mean') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute similarity matrix between all pairs of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to compare\n",
    "            pooling_strategy: Pooling strategy for embeddings\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of shape (len(texts), len(texts)) containing similarities\n",
    "        \"\"\"\n",
    "        # Get embeddings for all texts\n",
    "        embeddings = self.get_embeddings(texts, pooling_strategy=pooling_strategy)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings_normalized = embeddings / norms\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = np.dot(embeddings_normalized, embeddings_normalized.T)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def plot_similarity_matrix(self, texts: List[str], pooling_strategy: str = 'mean',\n",
    "                             figsize: tuple = (12, 10), annot: bool = True):\n",
    "        \"\"\"\n",
    "        Plot similarity matrix as a heatmap with proper Arabic text handling\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to compare\n",
    "            pooling_strategy: Pooling strategy for embeddings\n",
    "            figsize: Figure size for the plot\n",
    "            annot: Whether to annotate cells with numerical value\n",
    "        \"\"\"\n",
    "        similarity_matrix = self.compute_similarity_matrix(texts, pooling_strategy)\n",
    "        \n",
    "        # Create labels (keep full Arabic text, no truncation)\n",
    "        labels = texts.copy()  # Keep full text for Arabic\n",
    "        \n",
    "        # Create figure with Arabic-compatible settings\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create heatmap with right-to-left text alignment\n",
    "        ax = sns.heatmap(similarity_matrix, \n",
    "                        annot=annot, \n",
    "                        fmt='.2f',\n",
    "                        cmap='coolwarm', \n",
    "                        xticklabels=labels,\n",
    "                        yticklabels=labels,\n",
    "                        vmin=-1, \n",
    "                        vmax=1)\n",
    "        \n",
    "        # Rotate xlabels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        # Adjust label properties for Arabic text\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=10, ha='right')\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=10, ha='right')\n",
    "        \n",
    "        plt.title(f'Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†ØµÙŠ ({pooling_strategy})', fontsize=12, pad=20)  # Arabic title\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at BounharAbdelaziz/xlm-roberta-large-bs-16-lr-0.0001-ep-1-wp-0.1-gacc-8-gnm-1.0-FP16-mx-512-v0.1 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings...\n",
      "\n",
      "Generating similarity matrix with mean pooling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical similarity matrix (mean pooling):\n",
      "[[0.99999994 0.99174446 0.9909823  0.98841196]\n",
      " [0.99174446 1.0000001  0.98677593 0.98960155]\n",
      " [0.9909823  0.98677593 0.9999999  0.9872966 ]\n",
      " [0.98841196 0.98960155 0.9872966  1.0000004 ]]\n",
      "\n",
      "Generating similarity matrix with cls pooling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical similarity matrix (cls pooling):\n",
      "[[1.         0.9968043  0.99470186 0.993513  ]\n",
      " [0.9968043  1.0000001  0.99329454 0.99404615]\n",
      " [0.99470186 0.99329454 1.0000001  0.9928244 ]\n",
      " [0.993513   0.99404615 0.9928244  1.        ]]\n",
      "\n",
      "Generating similarity matrix with max pooling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical similarity matrix (max pooling):\n",
      "[[1.0000001  0.97399235 0.9489736  0.95984066]\n",
      " [0.97399235 1.         0.9390589  0.9548985 ]\n",
      " [0.9489736  0.9390589  1.         0.9513192 ]\n",
      " [0.95984066 0.9548985  0.9513192  1.0000002 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedder\n",
    "embedder = MaskedLMEmbedder(MODEL)\n",
    "\n",
    "# Example texts (in Arabic)\n",
    "texts = [\n",
    "    \"Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ Ù‡ÙŠ Ø§Ù„Ø±Ø¨Ø§Ø·\",  # Hello, how are you?\n",
    "    \" Ø§Ù„Ù…ØºØ±Ø¨ Ø¨Ù„Ø§Ø¯ Ø²ÙˆÙŠÙ†\",      # Good morning\n",
    "    \"Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† ÙØ§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†\",\n",
    "    \"Ø³ÙŠØ± ØªØ®Ø±Ø§ Ø§ Ø¯Ø§Ùƒ Ù„Ø­Ù…Ø§Ø±\", # I am at home\n",
    "]\n",
    "\n",
    "# Get embeddings with different pooling strategies\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "\n",
    "# Generate and plot similarity matrices for different pooling strategies\n",
    "for strategy in ['mean', 'cls', 'max']:\n",
    "    print(f\"\\nGenerating similarity matrix with {strategy} pooling...\")\n",
    "    # embedder.plot_similarity_matrix(texts, pooling_strategy=strategy)\n",
    "    \n",
    "    # Print numerical similarity matrix\n",
    "    sim_matrix = embedder.compute_similarity_matrix(texts, pooling_strategy=strategy)\n",
    "    print(f\"\\nNumerical similarity matrix ({strategy} pooling):\")\n",
    "    print(sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.98G/1.98G [00:53<00:00, 36.6MB/s]   \n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:00<00:00, 34.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def load_and_push(local_path, repo_id):\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = AutoModel.from_pretrained(local_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "    \n",
    "    # Push to hub\n",
    "    model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "falcon = \"/home/infres/abounhar/AtlasIA/to_my_github/Al-Atlas-Dataset/models/BounharAbdelaziz/Falcon3-1B-Base-bs-4-lr-0.0001-ep-2-wp-0.1-gacc-32-gnm-1.0-FP16-mx-2048-v0.1/checkpoint-7050\"\n",
    "qwen = \"/home/infres/abounhar/AtlasIA/to_my_github/Al-Atlas-Dataset/models/BounharAbdelaziz/Qwen2.5-0.5B-bs-4-lr-0.0001-ep-2-wp-0.1-gacc-32-gnm-1.0-FP16-mx-2048-v0.1/checkpoint-14550\"\n",
    "\n",
    "model_path = qwen\n",
    "model, tokenizer = load_and_push(\n",
    "    model_path,\n",
    "    \"atlasia/Al-Atlas-LLM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: /home/infres/abounhar/AtlasIA/to_my_github/Al-Atlas-Dataset/models/BounharAbdelaziz/Qwen2.5-0.5B-bs-4-lr-0.0001-ep-2-wp-0.1-gacc-32-gnm-1.0-FP16-mx-2048-v0.1/checkpoint-14550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "falcon = \"/home/infres/abounhar/AtlasIA/to_my_github/Al-Atlas-Dataset/models/BounharAbdelaziz/Falcon3-1B-Base-bs-4-lr-0.0001-ep-2-wp-0.1-gacc-32-gnm-1.0-FP16-mx-2048-v0.1/checkpoint-7050\"\n",
    "qwen = \"/home/infres/abounhar/AtlasIA/to_my_github/Al-Atlas-Dataset/models/BounharAbdelaziz/Qwen2.5-0.5B-bs-4-lr-0.0001-ep-2-wp-0.1-gacc-32-gnm-1.0-FP16-mx-2048-v0.1/checkpoint-14550\"\n",
    "\n",
    "model_path = qwen\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create pipeline\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_path,\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    # \"Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ Ø§Ù„Ù…ØºØ±Ø¨\",  # Hello, how are you?\n",
    "    # \" Ø§Ù„Ù…ØºØ±Ø¨ Ø¨Ù„Ø§Ø¯ Ø²ÙˆÙŠÙ†\",      # Good morning\n",
    "    # \"Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† ÙØ§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†\",\n",
    "    # \"Ø³ÙŠØ± ØªØ®Ø±Ø§ Ø§ Ø¯Ø§Ùƒ Ù„Ø­Ù…Ø§Ø±\", # I am at home\n",
    "    # \"Ø§Ù„Ù…Ø§ÙƒÙ„Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ÙƒØªØ¹ØªØ¨Ø± Ù…Ù† Ø£Ø­Ø³Ù† Ø§Ù„Ù…Ø§ÙƒÙ„Ø§Øª ÙØ§Ù„Ø¹Ø§Ù„Ù…\",\n",
    "    # \"Ù‚Ù„Ø¨ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ…Ù‡Ø§ ØªØ´Ø¨Ù‡Â Ø§Ù„Ø¨Ù†Øª\",\n",
    "    # \" Ø¹Ø·ÙŠÙ†Ø§ Ø´Ø±Ø­ Ù…ÙØµÙ„ Ø¹Ù„Ù‰ ÙƒÙŠÙØ§Ø´\",\n",
    "    # \"Ø¹Ø·ÙŠÙ†ÙŠ Ø®Ù…Ø³Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¯ÙŠØ§Ù„ Ù…Ù…Ø«Ù„ÙŠÙ†Â Ù…ØºØ§Ø±Ø¨Ø©\",\n",
    "    \"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ù„ÙŠ ÙƒÙŠØ±ÙƒØ² \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ù„ÙŠ ÙƒÙŠØ±ÙƒØ² Ú¯Ø§Ø¹ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø¢Ù„Ø§Øª Ø§Ù„Ù„ÙŠ Ù‚Ø§Ø¯Ø±Ø© ØªØ¯ÙŠØ± Ù…Ù‡Ø§Ù… Ø§Ù„Ù„ÙŠ Ø¹Ø§Ø¯Ø© Ø®Ø§ØµÙ‡Ø§ Ø°ÙƒØ§Ø¡ Ø¨Ø´Ø±ÙŠØŒ Ø¨Ø­Ø§Ù„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·ØŒ Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ØŒ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§ØªØŒ ÙˆÙÙ‡Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§.\n",
      "\n",
      "ÙˆØ§Ø­Ø¯ Ù…Ù† Ø£Ù‡Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¯ÙŠØ§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ§Ù„ØµØ­Ø©. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ‚Ø¯Ø± ÙŠØ¹Ø§ÙˆÙ† Ø§Ù„Ø·Ø¨Ø© Ø¨Ø§Ø´ ÙŠØ´Ø®ØµÙˆ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø¨Ø¯Ù‚Ø© Ø£ÙƒØ¨Ø±ØŒ ÙŠØ£ÙˆØªÙˆÙ…Ø§ØªÙŠØ²ÙŠÙˆ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ©ØŒ ÙˆÙŠØ­Ø³Ù†Ùˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±Ø¶Ù‰. Ù…Ø«Ù„Ø§ØŒ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø¯ÙŠØ§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªÙ‚Ø¯Ø± ØªØ­Ù„Ù„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ø´ ØªÙƒØªØ§Ø´Ù Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© Ø§Ù„Ù„ÙŠ Ù…Ù…ÙƒÙ† Ù…Ø§ ØªÙƒÙˆÙ†Ø´ ÙˆØ§Ø¶Ø­Ø© Ù„Ù„ÙØ­Øµ Ø§Ù„Ø¨Ø´Ø±ÙŠ. Ø²ÙŠØ§Ø¯Ø© Ø¹Ù„Ù‰ Ù‡Ø§Ø¯Ø´ÙŠØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ‚Ø¯Ø± ÙŠØ¹Ø§ÙˆÙ† Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø¨Ø§Ø´ ÙŠØµØ§ÙˆØ¨Ùˆ Ø®Ø·Ø· Ø¹Ù„Ø§Ø¬ Ù…Ø®ØµØµØ© Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¯ÙŠØ§Ù„ ÙƒÙ„ Ù…Ø±ÙŠØ¶.\n",
      "\n",
      "Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù„Ù„ØµØ­Ø©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù„ÙŠ ÙƒÙ†ØªÙØ§Ø¹Ù„Ùˆ Ø¨ÙŠÙ‡Ø§ Ù…Ø¹ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§. Ù…Ø«Ù„Ø§ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ‚Ø¯Ø± ÙŠØ¹Ø§ÙˆÙ† Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø¨Ø§Ø´ ÙŠØ­Ø³Ù†Ùˆ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¯ÙŠØ§Ù„Ù‡Ù…ØŒ ÙŠØ£ÙˆØªÙˆÙ…Ø§ØªÙŠØ²ÙŠÙˆ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ©ØŒ ÙˆÙŠØ­Ø³Ù†Ùˆ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø²Ø¨ÙˆÙ†. Ø²ÙŠØ§Ø¯Ø© Ø¹Ù„Ù‰ Ù‡Ø§Ø¯Ø´ÙŠØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ‚Ø¯Ø± ÙŠØ¹Ø§ÙˆÙ† Ø§Ù„ÙÙ„Ø§Ø­Ø© Ø¨Ø§Ø´ ÙŠØ²ÙŠØ¯Ùˆ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©ØŒ ÙŠÙ†Ù‚ØµÙˆ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙØŒ ÙˆÙŠØ­Ø³Ù†Ùˆ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„.\n",
      "\n",
      "ÙØ§Ù„Ø®ØªØ§Ù…ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ·ÙˆÙŠØ± Ø£Ù†Ø¸Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙ‚Ø¯Ù…Ø©ØŒ Ù†Ù‚Ø¯Ø±Ùˆ Ù†Ø­Ø³Ù†Ùˆ Ø§Ù„ÙƒÙØ§Ø¡Ø©ØŒ Ù†Ø­Ø³Ù†Ùˆ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø­ÙŠØ§Ø©ØŒ ÙˆÙ†Ø®Ù„Ù‚Ùˆ Ø¹Ø§Ù„Ù… Ø£Ø­Ø³Ù† Ù„Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ø¬Ø§ÙŠØ©. Ù…Ø¹ Ø§Ø³ØªÙ…Ø±Ø§Ø± ØªØ·ÙˆØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù…Ù† Ø§Ù„Ù…Ø±Ø¬Ø­ Ø£Ù†Ù‡ ØºØ§Ø¯ÙŠ ÙŠÙ„Ø¹Ø¨ Ø¯ÙˆØ± Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© ÙØªØ´ÙƒÙŠÙ„ Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. ÙˆÙ„ÙƒÙ†ØŒ Ø®Ø§ØµÙ†Ø§ Ù†ÙƒÙˆÙ†Ùˆ ÙˆØ§Ø¹ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø´ Ù†Ø³ØªØ§ÙØ¯Ùˆ Ù…Ù†Ùˆ Ø¨Ø£Ø­Ø³Ù† Ø·Ø±ÙŠÙ‚Ø©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. ÙˆÙ„ÙƒÙ†ØŒ Ø®Ø§ØµÙ†Ø§ Ù†ÙƒÙˆÙ†Ùˆ ÙˆØ§Ø¹ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø´ Ù†Ø³ØªØ§ÙØ¯Ùˆ Ù…Ù†Ùˆ Ø¨Ø£Ø­Ø³Ù† Ø·Ø±ÙŠÙ‚Ø©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. ÙˆÙ„ÙƒÙ†ØŒ Ø®Ø§ØµÙ†Ø§ Ù†ÙƒÙˆÙ†Ùˆ ÙˆØ§Ø¹ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø´ Ù†Ø³ØªØ§ÙØ¯Ùˆ Ù…Ù†Ùˆ Ø¨Ø£Ø­Ø³Ù† Ø·Ø±ÙŠÙ‚Ø©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. ÙˆÙ„ÙƒÙ†ØŒ Ø®Ø§ØµÙ†Ø§ Ù†ÙƒÙˆÙ†Ùˆ ÙˆØ§Ø¹ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø´ Ù†Ø³ØªØ§ÙØ¯Ùˆ Ù…Ù†Ùˆ Ø¨Ø£Ø­Ø³Ù† Ø·Ø±ÙŠÙ‚Ø©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§. ÙˆÙ„ÙƒÙ†ØŒ Ø®Ø§ØµÙ†Ø§ Ù†ÙƒÙˆÙ†Ùˆ ÙˆØ§Ø¹ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø´ Ù†Ø³ØªØ§ÙØ¯Ùˆ Ù…Ù†Ùˆ Ø¨Ø£Ø­Ø³Ù† Ø·Ø±ÙŠÙ‚Ø©. ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ø´ ÙŠØºÙŠØ± Ø¨Ø²Ø§Ù ÙƒÙŠÙØ§Ø´ ÙƒÙ†Ø¹ÙŠØ´ÙˆØŒ Ù†Ø®Ø¯Ù…ÙˆØŒ ÙˆÙ†ØªÙØ§Ø¹Ù„Ùˆ Ù…Ø¹ Ø¨Ø¹Ø¶ÙŠØ§ØªÙ†Ø§\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "for prompt in texts:\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        pad_token_id=generator.tokenizer.pad_token_id or generator.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.5,\n",
    "        num_beams=8,\n",
    "        # temperature=0.8,\n",
    "        top_p= 0.9,\n",
    "        top_k= 150,\n",
    "        do_sample= True,\n",
    "        early_stopping = True,\n",
    "    )[0]['generated_text']\n",
    "    print(f'outputs: {outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ Ù‡ÙŠ Ø§Ù„Ø±Ø¨Ø§Ø·. ÙƒØ§ÙŠÙ† ÙØ´Ø§Ø±Ø¹ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø®Ø§Ù…Ø³ ÙˆØ­ÙŠ Ø¨ÙˆØ±ÙƒÙˆÙ†ØŒ Ø´Ø§Ø±Ø¹ÙŠ Ø¹Ù„Ø§Ù„ Ø§Ù„ÙØ§Ø³ÙŠ ÙˆØ§Ù„Ø­Ø³Ù† Ø§Ù„ØªØ§Ù†ÙŠ.\n",
      "Ø¥Ø°Ù† Ø§Ù„Ø¬ÙˆØ§Ø¨ Ù‡Ùˆ: Ù„Ø§. Ù‡Ø§Ø¯ Ø§Ù„Ø´ÙŠ Ù…Ø§Ø´ÙŠ Ù…Ù…ÙƒÙ† ÙŠÙˆÙ‚Ø¹ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø´ÙŠ Ø­Ø§Ø¬Ø© ÙØ§Ù„Ø­Ù‚ÙŠÙ‚Ø©! Ú¯ÙˆÙ„ Ù„ÙŠØ§ ÙˆØ§Ø´ Ù†Ù‚Ø¯Ø± Ù†Ø¹Ø§ÙˆÙ†ÙƒØŸ\n",
      "Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯: \"ÙˆØ§Ø­Ø¯ Ù…Ù† Ø£Ø¨Ø±Ø² Ø§Ù„Ù…Ø¯Ù† ÙÙŠ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§.\"\n",
      "\"Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ø§ Ø£ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ø¯ÙŠØ§Ù„ Ø§Ù„Ù†Ø§Ø³ ÙÙŠÙ‡Ø§.\" ØµØ­ÙŠØ­ ÙˆÙ„Ø§ ØºÙ„Ø·ØŸ\n",
      "\n",
      "Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù‚ØµØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©:\n",
      "Ù‡Ø§Ø¯ÙŠ Ù‚ØµØ© Ø®Ø±Ø§ÙÙŠØ© Ø¹Ù„Ù‰ ÙˆØ§Ø­Ø¯ Ø§Ù„Ø³Ø§Ø­Ø± Ø³Ù…ÙŠØªÙˆ\n",
      "outputs:  Ø§Ù„Ù…ØºØ±Ø¨ Ø¨Ù„Ø§Ø¯ Ø²ÙˆÙŠÙ† Ø¨Ø²Ø§Ù. ÙƒØ§ÙŠÙ†Ø© ÙÙˆØ³Ø· Ø¥ÙØ±ÙŠÙ‚ÙŠØ§ØŒ ÙˆÙƒØªØ¶Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¯Ù† ÙˆØ§Ù„Ù‚Ø±Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… ØªØ§Ø±ÙŠØ® ØºÙ†ÙŠ ÙˆÙ…ØªÙ†ÙˆØ¹.\n",
      "\n",
      "ÙˆØ§Ø­Ø¯ Ù…Ù† Ø£Ù‡Ù… Ø§Ù„Ø­Ø¬Ø±Ø§ÙŠØ§Øª ÙØ§Ù„Ø¨Ù„Ø§Ø¯ Ù‡ÙŠ Ù…Ø±Ø§ÙƒØ´ØŒ Ø§Ù„Ø¹Ø§ØµÙ…Ø© ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© ÙØ£ÙˆØ±ÙˆØ¨Ø§ Ù„ÙŠ ØªØ³Ù…Ø§Øª Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø£Ù…Ø§Ø²ÙŠØºÙŠØ© \"Ù…Ø±Ø§ÙƒØ´ÙŠ\". Ù‡Ø§Ø¯ÙŠ Ù…Ø¯ÙŠÙ†Ø© ÙƒØ¨ÙŠØ±Ø© ÙÙŠÙ‡Ø§ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© ÙˆØ§Ù„Ø£Ø«Ø± Ø§Ù„Ø«Ù‚Ø§ÙÙŠ ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø§Ù„ÙÙ†ÙŠ.\n",
      "Ù…Ù† Ø¨ÙŠÙ† Ø£Ø´Ù‡Ø± Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø³ÙŠØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø´Ù‡ÙˆØ±Ø© ÙÙŠÙ† ÙŠÙ…ÙƒÙ† ØªØ´ÙˆÙÙ‡Ø§:\n",
      "\n",
      "1. Ø¬Ø§Ù…Ø¹ Ø§Ù„ÙÙ†Ø§:\n",
      "outputs: Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† ÙØ§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†. Ø£Ù†Ø§ Ø¹Ø¶ÙˆØ© ÙØ¬Ù…Ø¹ÙŠØ© â€œØ§Ù„Ø³Ù„Ø§Ù… ÙˆØ§Ù„Ø­Ø±ÙŠØ©â€ Ù„ÙŠ ØªØ£Ø³Ø³Øª Ø¨Ø§Ø´ Ù†Ø¯Ø§ÙØ¹ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† ÙˆØ§Ù„Ø­Ø±ÙŠØ§Øª Ø§Ù„ÙØ±Ø¯Ø§Ù†ÙŠØ©.\n",
      "\n",
      "Ø£Ù†Ø§ Ù…Ù† Ø§Ù„Ù†Ø§Ø³ Ø§Ù„Ù…ØºØ§Ø±Ø¨Ø© Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ù…Ø´ÙƒÙ„ Ù…Ø¹ Ø§Ù„Ø¸Ù„Ù… Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¯ÙˆÙ„Ø© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© ÙˆÙ„Ø§ Ø­ÙŠØª Ù…Ø§Ø´ÙŠ Ø£ØµÙ„ÙŠÙŠÙ† Ø¨Ø²Ø§Ù (Ø¨Ø­ÙƒÙ… Ø£Ù†Ù†ÙŠ Ø¬Ø¨Øª ÙˆÙ„Ø¯ÙŠ Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„). Ù‡Ø§Ø¯ Ø§Ù„Ø´ÙŠ Ø®Ù„Ø§ÙˆÙ†ÙŠ Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù„Ù„Ø§Ù†ØªÙ‚Ø§Ù… Ø£Ùˆ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ Ø¨Ø§Ù„Ù‚ØªÙ„ Ø¶Ø¯ Ø£ÙŠ ÙˆØ§Ø­Ø¯ ÙŠØ­Ø§ÙˆÙ„ ÙŠØªØ¨Ø¹ Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†.\n",
      "ÙˆÙ„ÙƒÙ† Ø±ØºÙ… ÙƒÙ„ Ù‡Ø°Ø§ØŒ ÙƒÙ†ØªÙ…Ù†Ù‰ ØªÙƒÙˆÙ†Ùˆ ÙƒØ§Ù…Ù„ÙŠÙ†\n",
      "outputs: Ø³ÙŠØ± ØªØ®Ø±Ø§ Ø§ Ø¯Ø§Ùƒ Ù„Ø­Ù…Ø§Ø± Ø±Ø§Ù‡ Ù…Ø±ÙŠØ¶ Ø¨Ø²Ø§Ù Ùˆ ÙƒÙŠØ¯ÙŠØ± ØºÙŠØ± Ø§Ù„Ø®ÙŠØ± ÙÙŠ Ø§Ù„Ù†Ø§Ø³. Ù‡Ø§Ø¯Ø´ÙŠ Ù„ÙŠ Ø®Ø§ØµÙˆ ÙŠØªØ¹Ø§Ù‚Ø¨ Ø¹Ù„ÙŠÙ‡ Ù‡Ùˆ Ø§Ù„Ù…Ø®Ø²Ù† Ù…Ø§Ø´ÙŠ Ù†Ø§Ø³ Ø¹Ø§Ø¯ÙŠÙŠÙ†.Ø§Ù„Ù„Ù‡ ÙŠØ³ØªØ± Ù…Ù† Ø¹Ù†Ø¯ÙƒÙ… ÙˆØ§Ù„Ù‡ ÙŠØ¬Ø¹Ù„Ù‡Ø§ ÙØ§ØªØ­Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ù„ Ø§Ù„Ù‡ ÙŠØ§Ø®Ø¯ Ø§Ù„Ø­Ù‚ ÙØ§Ù„Ù…Ù†Ø§ÙÙ‚ÙŠÙ† Ø¯ÙŠÙ…Ø§ ØªØ§Ø¨Ø¹ÙŠÙ†Ù‡Ù… Ø­ØªÙ‰ ÙˆØ§Ø­Ø¯ Ù„Ø§Ø®Ø± ÙˆÙ„Ø§ÙƒÙ† Ø­Ù†Ø§ Ù…Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø´ÙƒÙ„ Ø§Ù†ØªÙ…Ø§ ØºØªØ¨Ù‚Ø§Ùˆ ØªÙ†ÙˆØ¶ÙˆØ§ Ø¹Ù„ÙŠÙ†Ø§ Ø¨Ø§Ù„Ù„ÙŠ Ø¨ØºÙŠÙ†Ø§ Ø´ÙŠ Ø­Ø§Ø¬Ø© ÙˆÙ„ÙƒÙ† Ø¨Ù„Ø§ Ø³Ù†Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§ Ø§Ùˆ Ø¬Ù…Ø¹ÙŠØ§ØªÙŠØ© Ø£Ùˆ Ø£ÙŠ Ø·Ø±Ù Ø¢Ø®Ø± Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØªØ¨Ø±ÙƒÙŠÙƒØ© Ù‡ÙŠ\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "for prompt in texts:\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=generator.tokenizer.pad_token_id or generator.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.3\n",
    "    )[0]['generated_text']\n",
    "    print(f'outputs: {outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ Ù‡ÙŠ Ø³Ù„Ø§. ÙƒØªØ¨Ù‡Ùˆ Ø§Ù„Ø£ØºÙ†ÙŠØ© \"Ø§Ù„Ø±Ø¦ÙŠØ· ÙˆØ§Ù„Ø£Ø­Ø¯Ø§Ù†\".\n",
      "ÙÙŠÙ† Ù…Ø´Ù‰ Ù„ÙƒÙˆÙ†Ø· ÙÙ‚ÙŠØ± Ø¨Ø£ØµÙˆÙ„ Ø£Ø®Ø¶Ø§Ø±ØŒ Ø­Ø°Ø§Øª ØªØ¬Ø§Ù…ÙˆØ² Ø±Ø§Ø¯ÙˆØ± Ø¬Ø§Ø¨ ÙŠØ¨Ø±Ø§Ø¡ Ù†Ú¯Ø± Ø´Ù‘ÙÙ€Ù’Ù Ø§ÙØ«ÙÙ‘â€¢ Ø¥ÙÙ‘Ù† Ù‚ÙØ§Ù„:Â«..Â»...!\n",
      "- Ø§Ø®ØªØ§Ø± Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù„Û¢Ú­ÛŒ Ù¾İ¥Ü¡İ¤İ İ™İİİšİ›İ”İ•İ‰İİİİ‹İŠİİˆİ…İ‡İ†İ„İ‚İƒİİ—İ˜İ“İ–İŸİœİŒİ‘İ’İ¯İ«İ°İ©İªİ´İ§İ¸İ¨İ±İ»İ¶İ¼İ½İ¿İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£İ£ï¿½\n",
      "--------------------------------------------------\n",
      "outputs:  Ø§Ù„Ù…ØºØ±Ø¨ Ø¨Ù„Ø§Ø¯ Ø²ÙˆÙŠÙ†Ø© ÙˆÙ…Ø§ ÙƒØªØ¹Ø±ÙØ´. Ø§Ù„Ø¬ÙŠØ¯Ø§Ù† Ø¯Ø§Ø± Ù„Ù‡ Ù…Ù†Ø·Ù‚ÙŠØ© ÙØ§Ù„ØµØ¨Ø§Ø­ØŒ Ø­ÙŠØ« ØªØ­ÙˆÙ„Ø§Øª Ø§Ù•ØµØ§Ø¨Ø§Øª Ø¬Ø«Ø§Ù… 1036 Ù†Ø¬Ù…Ø§Ø¡ Ù‚Ø¶Ù‰ Ø§Ø³ØªØºØ§Ù„Ùƒ Ø±ÙˆØ¶ ÙŠÙˆÙ†Ú¯ Ø´Ø®Øµ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±ÙˆØ±Ø©.. Ø¥ÙŠØ±Ø§Ù†: Ø£Ù†Ø§ Ù…Ø£Ù‘Ù’Ø±ÙÙŠÙÙ‘ÙÙ...\n",
      "Q: ÙˆØ§Ø´ Ø§Ù„Ù…Ø¤Ø³Ø³Ø© Ø§Ù„ØªØ§Ù„Ø«Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¸Ù„Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ù„â€Ø§Ù„Ø£Ø¢Ù„Ø©â€: â€œØ§Ù„Ù…Ø·Ù„Ø¨â€.\n",
      "A: A post shared by Irena on Apr 27, 2024 at 8:59am PDT\n",
      "B: A post shared by Irena on May 1st, 2024 at 1pm EDT\n",
      "C: A post shared by Irena on April 2nd, 2024 at 11:59am CDT\n",
      "D: None of the above choices are correct!\n",
      "E: All of these could be trueâ€¦what do you think?\n",
      "F: A post shared by Irena on March 27th, 2024 at 11:59am MDT\n",
      "G: A post shared by Irena on February 27th, 2024 at 11:59am MST\n",
      "H: A post shared by Irena on January 27th, 2024 at 11:59am MTJ\n",
      "I: A post shared by Irena on December 27th, 2023 at 11:59AM MDT\n",
      "K: A post shared by Irena on November 27th, 2023 at 11:59 AM ET\n",
      "L: A post shared by Irena on October 27th, 2023 at 11:59AM CTM\n",
      "N: A post shared by Irena on September 27th, 2023 at 11:59AM CST\n",
      "O: A post shared by Irena on August 27th, 2023 at 11:59AM DTZ\n",
      "P: A post shared by Irena on July\n",
      "--------------------------------------------------\n",
      "outputs: Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ù…Ø±ÙŠÙ…ØŒ Ùˆ ÙƒÙ†Ø³ÙƒÙ† ÙØ§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø¯ÙÙ„Ø³Ø·ÙŠÙ†.\n",
      "Ø§Ù„Ø¬ÙˆØ§Ø¨ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø«ÙŠØ±Ø©: Ù„Ø§ Ù†Ø­ÙˆÙ„ Ø¥Ø´Ø§Ø±Ø§Øª Ø£Ø®Ø¨Ø§Ø±ÙŠØ© Ø¨Ø£Ø¬ÙˆØ± ØªØµØºÙŠØ± - Ø¬ÙˆÙ† Ø´ÙŠØ®ÙˆØ² Ø­Ø§Ù…Ø¶Ø§Ø¡ Ø±Ø§Ø¯ÙŠÙˆ ÙŠØ¨Ø±Ù‰ Ù‚Ø±Ø¨ Ø§Ø¨ Ø§Ù”Ø«Ù†Ø§Ø¡ Ø·Ø±ÙŠÙ‚Ø© ØªØµØºÙŠØ± Ø§Ù„Ú¯Ø±ÙŠÚ¤ÙŠÙˆ..\n",
      "--\n",
      "Q134567890:: A post shared by Maryam on Jul 2, 2019 at 1:10pm PDT...\n",
      "Apostle of the Church in America!\n",
      "https://www.facebook.com/pages/.../Mary%C3%BByram#page=521100000000000&sid=-14bcfbfaffaecfeebfbdccdcbceeaeeacbeabdaeedcdebdeedeâ€¦ https://twitter.com/#!/marybyram/?sId=&id_src=%E2%AF%BF%DABFBFCFBEFEFAECDAEAACBDCAEDFFCCDEBAAD http://bitly.co/YJZlRj https://youtu.be/_gKqWpPzIhM?t=_GkUuOaLwTâ€” Mary Bymar May 2, 2019 at 1:10 pm EDT\n",
      "---\n",
      "@mariamayborama I'm so grateful for your support and prayers during this time!! â¤ï¸â¤ï¸ğŸ™ğŸ» ğŸ™„ğŸ¼ğŸ’™ pic.twitter.com/yVvXoHxNnS â€“ Mariya ByrmĞ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¶Ğ°Ñ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¿Ğ¸ĞºÑƒ ÑĞ°Ğ±Ñ‹Ñ‡ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ·Ğ£Ñ‡ĞµĞ±ĞĞµÑÑ‚Ğ´Ñ†ĞºĞ¸Ğµ ĞºĞ»ÑĞ±ĞºĞ°. ĞŸÑ€Ğ¸Ğ°Ğ¶ĞµÑ‚ÑÑ ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ñ€ÑƒĞ·ÑŒÑ. Ğ¡Ğ¿Ğ°ÑĞ°Ñ‚ÑŒ Ğ’Ğ°Ñˆ ĞºĞ­Ù„ Ù¡ï¸ğŸš©ğŸ˜ğŸ¤—ğŸ˜‚ğŸ‘ŒğŸ‰ğŸ¥ğŸ…ğŸ•ğŸŒ¸ğŸ“ˆâœ¨ğŸ‡ªğŸ‡¦ ğŸ‡§ğŸ‡› ğŸ–âœ‹â˜€ï¸ğŸ”¥ğŸ”¥âš ï¸â™»ï¸âƒŸï¸â­ï¸ğŸš€ ğŸŒ ğŸ¤ ğŸ¤¯â¤ â¤\n",
      "--------------------------------------------------\n",
      "outputs: Ø³ÙŠØ± ØªØ®Ø±Ø§ Ø§ Ø¯Ø§Ùƒ Ù„Ø­Ù…Ø§Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†Øª ÙˆØ§Ù„Ùˆ Ù…Ø§Ø´ÙŠ Ø¨Ø§Ù„Ø¶Ù‡ÙˆØ±.\n",
      "Ø´ÙØ· Ø§Ù„Ø­Ù‚Ø¯ ÙÙŠÙ† ÙƒØ§Ù† Ø´Ø§Ø¯ Ø±Ø¬Ø§Ø¡ Ø¬Ù…ØºÙˆÙ„ÙŠØ© ÙŠØ®Øµ Ù†Ø¶Ø±Ø¨ Ù‡Ø°Ø§ Ø­Ø§Ù…Ù„ ÙŠÙˆØ²ÙˆÙ†ØŒ Ù‚Ø§Ø¨ Ø§Ø¨ØºØ§Øª Ø£Ø«Ø± Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… ØµÙ€Ù°Ú¯Ú¤ÙŠ: â€œØ£Ù†Ø§â€..\n",
      "10/25/2019|3684 views\n",
      "- \"Ø§Ù„Ø£Ø¸Ù‡Ø±...\".\n",
      "7/25/2019|3684 views\n",
      "- \"... ÙˆØ§Ø¬Ù‡Ø©...\"!\n",
      "7/25/2019|3684 views\n",
      "- \".. ÙˆØ§Ø·Ù„Ø¹!\"\n",
      "7/25/2019|3684 views\n",
      "- \"\".....\"\n",
      "7/25/2019|3684 views\n",
      "- \"!.....!\".\n",
      "7/25/2019|3684 views\n",
      "- \"*......*..\"\n",
      "7/25/2019|3684 views\n",
      "- \"#.......#.\".\n",
      "7/25/2019|3684 views\n",
      "- \"-........-\"\n",
      "7/25/2019|3684 views\n",
      "- \"**..........**....\"\n",
      "7/25/2019|3684 views\n",
      "- \":.........:\"\n",
      "7/25/2019|3684 views\n",
      "- \"[............]!!!\"\n",
      "7/25/2019|3684 views\n",
      "- [................].!!\n",
      "7/25/2019|3684 views\n",
      "- [\"...........\"]!!.\n",
      "7/25/2019|3684 views\n",
      "- [[...]!]!.\n",
      "7/25/2019|3684 views\n",
      "- (...)![.].!\n",
      "7/25/2019|3684 views\n",
      "- [(.)[!]].[.]?!\n",
      "7/25/2019|3684 views\n",
      "- (().)!.()!)!...\n",
      "7/25/201\n",
      "--------------------------------------------------\n",
      "outputs: Ø§Ù„Ù…Ø§ÙƒÙ„Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ÙƒØªØ¹ØªØ¨Ø± Ù…Ù† Ø£Ø­Ø³Ù† Ø§Ù„Ù…Ø§ÙƒÙ„Ø§Øª ÙØ§Ù„Ø¹Ø§Ù„Ù…. Ù‡Ø§Ø¯ Ø§Ù„Ø´ÙŠ Ø®Ø·ÙˆØ© Ø¨Ø®ØµØ§Ø¦Ø¯Ù‡:\n",
      "- ØªÙ‚Ø±ÙŠØ± Ø¯ÙŠÙ†Ø§Ù…ÙŠØ« Ù„Ø£ÙˆÙ„ Ø­ÙØ¶Ø§Ø¡ ÙˆØ¬Ø¨Ø§Ø± (2018)ØŒ Ù†Ø´Ø± Ø±Ø§Ø¨ÙˆØ± Ø¬ÙˆÙ† Ø´Ø§Ù†Øº Ø¥Ø° ÙŠÙˆØ²Ù‰ Ù‚Ø¨Ù„ Ø§Ù”Ø«Ù†Ø§Ø¡ Ø§Ø­ØªØ¬Ø§Ø¬Ù‡ ØµØ§Ø·Ø§ Ø¶Ø¯ Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ Ø§Ù„Ù…Ø¸Ù„Ù…..\n",
      "- The World Bank's Global Development Finance Report, Volume I and II â€“ A Review of the State of Financial Inclusion in Developing Countries by Mohamed Elhadidi et al., Oxford University Press, New York/Oxford, UK, pp. xvii+356 pages; ISBN978019421010X.\n",
      "- International Monetary Fund â€” Country Data for Central Asia [Internet]. Washington DC.: IMF.; n.d-. Available from http://www.imf.org/externalsite?redirect=home&srcid=hsc_en&_cid=_gctc&_cdi=&sid=%EAD%BFD%CDE%AADD%CFDB%DBCF%BFBD%CEAF%AEFA on June 2nd, 2019.\n",
      "- MladenoviÄ‡, Zoran () \"The Impact Of Covid On Economic Growth In Serbia And Montenegro\". Journal of Public Economics Vol. 12 No. 1 p. 1-16 https:/doi. org/10.1016/j. jpubeco. 2020.02.002\n",
      "|ØªÙ‚Ø¯Ø± ØªÚ¯Ú¾ÛŒÙ„ÙŠ Ù±Ù„Û•Ø¥ÙÙ‘ÙÙ’ÙÙÙ‘ÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙâ€¢||ØªÙ‚Ø¯Ø± Øªİ£ÙˆÙ„.|\n",
      "|ØªÙ‚Ø¯Ø± Øªİ¢Ø¤Ù„ÙŠ Ù°Ù„Û™Ù‘ÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙÙ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "for prompt in texts:\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=generator.tokenizer.pad_token_id or generator.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.5,\n",
    "        # num_beams=4,\n",
    "    )[0]['generated_text']\n",
    "    print(f'outputs: {outputs}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
